{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXpY26ZQlX0L",
        "outputId": "4dc7b694-b911-4d52-e50c-819f8f593360"
      },
      "source": [
        "#importing dependencies\n",
        "import numpy\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp24JCTORA_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4f36239-72c9-4adc-9230-48f900f37779"
      },
      "source": [
        "!wget https://www.gutenberg.org/files/84/84-0.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-27 14:33:39--  https://www.gutenberg.org/files/84/84-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 448821 (438K) [text/plain]\n",
            "Saving to: ‘84-0.txt’\n",
            "\n",
            "84-0.txt            100%[===================>] 438.30K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-09-27 14:34:16 (4.01 MB/s) - ‘84-0.txt’ saved [448821/448821]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhxWeSvCrtIJ"
      },
      "source": [
        "#loading data \n",
        "# loading data and opening our input data in the form of a txt file\n",
        "#project Gutenberg/berg is where the data can be found \n",
        "file = open(\"84-0.txt\").read()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-5nKc0jr3oZ"
      },
      "source": [
        "#tookenisation\n",
        "#standardisation\n",
        "def tokenize_words(input):\n",
        "  # lowercase everything to a standardize it\n",
        "  input = input.lower()\n",
        "  # instantiating the tokenizer\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  # tokenizing the text into tokens\n",
        "  tokens = tokenizer.tokenize(input)\n",
        "  # filtering the stopwords using lambda\n",
        "  filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
        "  return \" \".join(filtered)\n",
        "\n",
        "# preprocess the input data, make tokens \n",
        "processed_inputs = tokenize_words(file)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYbmUtjJsq5S"
      },
      "source": [
        "# chars to numbers \n",
        "chars = sorted(list(set(processed_inputs)))\n",
        "char_to_num = dict((c,i) for i, c in enumerate(chars))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUshQ5FktB4R",
        "outputId": "ac31dedf-5e77-4cdc-dd63-2e7f9efa0037"
      },
      "source": [
        "# Check if words to chars or chars  to num(?!) has worked?\n",
        "input_len = len (processed_inputs)\n",
        "vocab_len = len (chars)\n",
        "print(\"Total number of characters:\", input_len)\n",
        "print(\"Total vocab:\", vocab_len)\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 269878\n",
            "Total vocab: 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQexQ3JPtNXw"
      },
      "source": [
        "#seq length\n",
        "seq_length = 100\n",
        "x_data = []\n",
        "y_data = []"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtcA8UTztajR",
        "outputId": "d3fa5183-8eb7-4318-e452-f586d6df20ed"
      },
      "source": [
        "#loop through the sequence \n",
        "for i in range(0, input_len - seq_length, 1):\n",
        "  in_seq = processed_inputs[i:i + seq_length] \n",
        "  out_seq = processed_inputs[i + seq_length]\n",
        "  x_data.append([char_to_num[char] for char in in_seq])\n",
        "  y_data.append(char_to_num[out_seq])\n",
        "\n",
        "n_patterns = len(x_data)\n",
        "print(\"Total Patterns:\", n_patterns)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns: 269778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLxoXjmSuwQp"
      },
      "source": [
        "#convert input sequence to np array and so on\n",
        "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
        "X = X/float(vocab_len)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXzYzmZ-u5cH"
      },
      "source": [
        "#one hot-encoding\n",
        "y = np_utils.to_categorical(y_data)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nGnbUxfu5t-"
      },
      "source": [
        "#creating the model\n",
        "model= Sequential()\n",
        "model.add(LSTM(256,input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXbw4QZMvPAi"
      },
      "source": [
        "#compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npGiPuS8vd74"
      },
      "source": [
        "#saving weights\n",
        "filepath = 'model_weights_saved.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose= 1, save_best_only=True, mode='min')\n",
        "desired_callbacks = (checkpoint)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8m3tg9nv05w",
        "outputId": "245f6d7e-0d59-4ff7-e194-a67ab7bf7daf"
      },
      "source": [
        "# fit model and let it train \n",
        "model.fit(X,y, epochs=4, batch_size=256, callbacks=desired_callbacks)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "1054/1054 [==============================] - 4271s 4s/step - loss: 2.9305\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.93051, saving model to model_weights_saved.hdf5\n",
            "Epoch 2/4\n",
            "1054/1054 [==============================] - 4295s 4s/step - loss: 2.6616\n",
            "\n",
            "Epoch 00002: loss improved from 2.93051 to 2.66161, saving model to model_weights_saved.hdf5\n",
            "Epoch 3/4\n",
            "1054/1054 [==============================] - 4015s 4s/step - loss: 2.5099\n",
            "\n",
            "Epoch 00003: loss improved from 2.66161 to 2.50990, saving model to model_weights_saved.hdf5\n",
            "Epoch 4/4\n",
            "1054/1054 [==============================] - 4358s 4s/step - loss: 2.4064\n",
            "\n",
            "Epoch 00004: loss improved from 2.50990 to 2.40641, saving model to model_weights_saved.hdf5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8f366c3050>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S89W2YpvpR6P"
      },
      "source": [
        "# recompile model with the saved weights\n",
        "filename = 'model_weights_saved.hdf5'\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVrJINtopnyA"
      },
      "source": [
        "# output of the model back into characters\n",
        "num_to_char = dict((i,c) for i , c in enumerate(chars))"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a33W1mY0pueY",
        "outputId": "b3e45c58-34ab-4b1d-a48b-f107965b1fa9"
      },
      "source": [
        "# random seed to help ganerate\n",
        "start = numpy.random.randint(0,len(x_data)-1)\n",
        "pattern = x_data[start]\n",
        "print(\"Random Seed :\")\n",
        "print(\"\\\"\",''.join([num_to_char[value] for value in pattern]),\"\\\"\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed :\n",
            "\" tions 50 states united states compliance requirements uniform takes considerable effort much paperwo \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDSh8hodp_GA",
        "outputId": "efa81865-81d2-43d4-bde6-476bacac8ddc"
      },
      "source": [
        "# generate the text\n",
        "for i in range(1000):\n",
        "  x = numpy.reshape(pattern, (1,len(pattern), 1))\n",
        "  x = x/float(vocab_len)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = num_to_char[index]\n",
        "  seq_in = [num_to_char[value] for value in pattern]\n",
        "  sys.stdout.write(result)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[ 1:len(pattern)]"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare se"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6P0L9GtJofC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}